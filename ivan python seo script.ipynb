{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65e333e-2c42-4811-9293-7929b08fe960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3892b14-4a20-4e17-bcf6-d1689674c787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 13:33:20,367 - INFO - Starting Search Console data extraction\n",
      "2024-10-19 13:33:20,367 - INFO - Reading credentials from: C:\\Users\\IAN NAMBOGA MADETE\\Downloads\\boxwood-chassis-438812-q4-da810f36cfed.json\n",
      "2024-10-19 13:33:20,367 - INFO - Temporary credentials file created at: C:\\Users\\IANNAM~1\\AppData\\Local\\Temp\\tmpmmwuplmi\n",
      "2024-10-19 13:33:20,411 - INFO - Credentials successfully created\n",
      "2024-10-19 13:33:20,416 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2024-10-19 13:33:20,417 - INFO - Search Console service successfully created\n",
      "2024-10-19 13:33:22,363 - INFO - Found 0 sites\n",
      "2024-10-19 13:33:22,363 - WARNING - No sites found in Search Console\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sites found in your Search Console account.\n"
     ]
    }
   ],
   "source": [
    "import google.auth\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def print_service_account_info(creds):\n",
    "    \"\"\"Print service account details for verification\"\"\"\n",
    "    try:\n",
    "        logging.info(\"=== Service Account Information ===\")\n",
    "        logging.info(f\"Service Account Email: {creds.service_account_email}\")\n",
    "        logging.info(f\"Project ID: {creds.project_id}\")\n",
    "        logging.info(\"=====================================\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not get service account details: {str(e)}\")\n",
    "\n",
    "def setup_credentials():\n",
    "    \"\"\"Set up Google credentials with error handling\"\"\"\n",
    "    try:\n",
    "        credentials_path = r'C:\\Users\\IAN NAMBOGA MADETE\\Downloads\\boxwood-chassis-438812-q4-da810f36cfed.json'\n",
    "        logging.info(f\"Reading credentials from: {credentials_path}\")\n",
    "        \n",
    "        with open(credentials_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as temp_file:\n",
    "            temp_file.write(json.dumps(data))\n",
    "            logging.info(f\"Temporary credentials file created at: {temp_file.name}\")\n",
    "            \n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            temp_file.name, \n",
    "            scopes=['https://www.googleapis.com/auth/webmasters.readonly']\n",
    "        )\n",
    "        logging.info(\"Credentials successfully created\")\n",
    "        return creds\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Credentials file not found at: {credentials_path}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        logging.error(\"Invalid JSON in credentials file\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error setting up credentials: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def get_search_console_service():\n",
    "    \"\"\"Build and return the Search Console service\"\"\"\n",
    "    try:\n",
    "        creds = setup_credentials()\n",
    "        service = build('searchconsole', 'v1', credentials=creds)\n",
    "        logging.info(\"Search Console service successfully created\")\n",
    "        return service\n",
    "    except HttpError as error:\n",
    "        logging.error(f'HTTP error occurred: {error}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error creating service: {str(e)}')\n",
    "        raise\n",
    "        \n",
    "def get_site_list(service):\n",
    "    \"\"\"Get list of sites in Search Console\"\"\"\n",
    "    try:\n",
    "        sites = service.sites().list().execute()\n",
    "        logging.info(f\"Found {len(sites.get('siteEntry', []))} sites\")\n",
    "        return sites\n",
    "    except HttpError as error:\n",
    "        logging.error(f'Error fetching sites: {error}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f'Unexpected error fetching sites: {str(e)}')\n",
    "        raise\n",
    "\n",
    "def get_leading_queries(service, site_url):\n",
    "    \"\"\"Get leading queries for each URL\"\"\"\n",
    "    try:\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        start_date = (datetime.now() - timedelta(days=28)).strftime('%Y-%m-%d')\n",
    "        logging.info(f\"Fetching data for period: {start_date} to {end_date}\")\n",
    "        \n",
    "        request = {\n",
    "            'startDate': start_date,\n",
    "            'endDate': end_date,\n",
    "            'dimensions': ['page', 'query'],\n",
    "            'rowLimit': 25000,\n",
    "            'startRow': 0\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Requesting data for site: {site_url}\")\n",
    "        response = service.searchanalytics().query(siteUrl=site_url, body=request).execute()\n",
    "        \n",
    "        if not response.get('rows'):\n",
    "            logging.warning(\"No data rows returned from Search Console\")\n",
    "            return []\n",
    "            \n",
    "        logging.info(f\"Received {len(response['rows'])} rows of data\")\n",
    "        \n",
    "        # Process the data as before...\n",
    "        results = []\n",
    "        url_data = {}\n",
    "        \n",
    "        for row in response['rows']:\n",
    "            url = row['keys'][0]\n",
    "            query = row['keys'][1]\n",
    "            clicks = row.get('clicks', 0)\n",
    "            impressions = row.get('impressions', 0)\n",
    "            \n",
    "            if url not in url_data:\n",
    "                url_data[url] = {\n",
    "                    'queries_clicks': [],\n",
    "                    'queries_impressions': []\n",
    "                }\n",
    "            \n",
    "            url_data[url]['queries_clicks'].append((query, clicks))\n",
    "            url_data[url]['queries_impressions'].append((query, impressions))\n",
    "        \n",
    "        for url, data in url_data.items():\n",
    "            clicks_sorted = sorted(data['queries_clicks'], \n",
    "                                key=lambda x: x[1], \n",
    "                                reverse=True)\n",
    "            impressions_sorted = sorted(data['queries_impressions'], \n",
    "                                     key=lambda x: x[1], \n",
    "                                     reverse=True)\n",
    "            \n",
    "            results.append({\n",
    "                'URL': url,\n",
    "                'Leading Query (Clicks)': clicks_sorted[0][0] if clicks_sorted and clicks_sorted[0][1] > 0 else \"-\",\n",
    "                'Leading Query (Impressions)': impressions_sorted[0][0] if impressions_sorted else \"-\"\n",
    "            })\n",
    "        \n",
    "        logging.info(f\"Processed {len(results)} URLs with their leading queries\")\n",
    "        return results\n",
    "        \n",
    "    except HttpError as error:\n",
    "        logging.error(f'Error fetching query data: {error}')\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f'Unexpected error processing queries: {str(e)}')\n",
    "        raise\n",
    "\n",
    "def export_to_excel(results, filename):\n",
    "    \"\"\"Export results to Excel file\"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_excel(filename, index=False)\n",
    "        logging.info(f\"Data successfully exported to {filename}\")\n",
    "    except Exception as error:\n",
    "        logging.error(f'Error exporting to Excel: {error}')\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"Starting Search Console data extraction\")\n",
    "        \n",
    "        service = get_search_console_service()\n",
    "        sites_data = get_site_list(service)\n",
    "        \n",
    "        if not sites_data.get('siteEntry'):\n",
    "            logging.warning(\"No sites found in Search Console\")\n",
    "            print(\"No sites found in your Search Console account.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nAvailable sites:\")\n",
    "        for site in sites_data['siteEntry']:\n",
    "            print(f\"- {site['siteUrl']}\")\n",
    "        \n",
    "        selected_site = input(\"\\nEnter the complete site URL from above: \").strip()\n",
    "        \n",
    "        if not any(site['siteUrl'] == selected_site for site in sites_data['siteEntry']):\n",
    "            logging.error(f\"Invalid site URL selected: {selected_site}\")\n",
    "            print(\"Error: Selected site URL does not match any available sites.\")\n",
    "            return\n",
    "        \n",
    "        results = get_leading_queries(service, selected_site)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No data found for the selected site.\")\n",
    "            return\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f'leading_queries_{timestamp}.xlsx'\n",
    "        export_to_excel(results, filename)\n",
    "        \n",
    "        print(f\"\\nProcessed {len(results)} URLs\")\n",
    "        print(f\"Results exported to: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Main execution failed: {str(e)}\")\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")\n",
    "        print(\"Check the logs for more details.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c504fb-a977-4d75-a6d0-88f383b53fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291c01a-a5d3-4397-bc6e-d00cf679bcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
